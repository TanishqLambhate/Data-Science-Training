{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc2060e-db25-495d-8e80-5b06cebb2151",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n|      1001|     John Doe|        HR| 2021-01-15| 58000|\n|      1009|  Sarah Adams| Marketing| 2021-09-01| 60000|\n|      1010|  Robert King|        IT| 2022-01-10| 62000|\n+----------+-------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/updated_employee_data.csv\", \"dbfs:/FileStore/employee_updates.csv\")\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_data.csv\", \"dbfs:/FileStore/employee_data.csv\")\n",
    "# Convert employee CSV data to Delta format\n",
    "df_employee=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/FileStore/employee_data.csv\")\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_data\")\n",
    "\n",
    "#convert employee updates csv data to delta\n",
    "df_employee_updates=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/FileStore/employee_updates.csv\")\n",
    "df_employee_updates.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_updates\")\n",
    "\n",
    "#Load Delta tables\n",
    "df_employee=spark.read.format(\"delta\").load(\"/delta/employee_data\")\n",
    "df_employee_updates=spark.read.format(\"delta\").load(\"/delta/employee_updates\")\n",
    "\n",
    "# Create temproray views for SQL operations\n",
    "df_employee.createOrReplaceTempView(\"delta_employee\")\n",
    "df_employee_updates.createOrReplaceTempView(\"employee_updates\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_employee AS target\n",
    "          USING employee_updates AS source\n",
    "          ON target.EmployeeID = source.EmployeeID\n",
    "          WHEN MATCHED THEN UPDATE SET target.Salary=source.Salary, target.Department=source.Department\n",
    "          WHEN NOT MATCHED THEN INSERT (EmployeeID,Name,Department , JoiningDate, Salary)\n",
    "          VALUES (source.EmployeeID, source.Name, source.Department, source.JoiningDate, source.Salary)\n",
    "\n",
    "          \"\"\")\n",
    "\n",
    "# Query the Delta table to check if the data was updated or inserted correctly\n",
    "spark.sql(\"SELECT * FROM delta_employee\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Merge Delta",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
