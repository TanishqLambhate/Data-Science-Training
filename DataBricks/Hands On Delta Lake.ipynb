{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721da183-5ece-463c-b697-ce3e6d10f1dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/sales_data.csv\", \"dbfs:/FileStore/sales_data1.csv\")\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/customer_data.json\")\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/new_sales_data.csv\", \"dbfs:/FileStore/new_sales_data.csv\")\n",
    "# 1.1 Load sales_data.csv into a DataFrame and Write as Delta Table\n",
    "# Load sales_data.csv into DataFrame\n",
    "sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/sales_data1.csv\")\n",
    "\n",
    "# Write the DataFrame as Delta Table\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sales_data1\")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "\n",
    "# Define schema for customer_data.json\n",
    "customer_schema = StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"SignupDate\", DateType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b5ee14-04a3-4d5c-ba0b-de909cd3af6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------+----------+\n",
      "|CustomerID| CustomerName|Region|SignupDate|\n",
      "+----------+-------------+------+----------+\n",
      "|      C001|     John Doe| North|2022-07-01|\n",
      "|      C002|   Jane Smith| South|2023-02-15|\n",
      "|      C003|Emily Johnson|  East|2021-11-20|\n",
      "|      C004|Michael Brown|  West|2022-12-05|\n",
      "|      C005|  Linda Davis| North|2023-03-10|\n",
      "+----------+-------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Load customer_data.json into a DataFrame and Write as Delta Table\n",
    "# Load the JSON data with the defined schema\n",
    "customer_df = spark.read.format(\"json\").schema(customer_schema).load(\"dbfs:/FileStore/customer_data.json\")\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8270ff-3dc2-4d1a-877f-870301b5dfd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame as Delta Table\n",
    "customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d5e59b-4aef-4554-b1aa-ae63d50488d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 Load new_sales_data.csv into a DataFrame and Write as Delta Table\n",
    "# Load new_sales_data.csv into DataFrame\n",
    "new_sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/new_sales_data.csv\")\n",
    "# Write the new DataFrame as Delta Table\n",
    "new_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/new_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13270b38-adb7-4df2-aa63-fbd93f6b53ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Load existing Delta table\n",
    "delta_table =spark.read.format(\"delta\").load(\"/delta/sales_data1\")\n",
    "delta_new_sales=spark.read.format(\"delta\").load(\"/delta/new_sales_data\")\n",
    "# Create temporary views for SQL operations\n",
    "delta_table.createOrReplaceTempView(\"delta_sales_data\")\n",
    "delta_new_sales.createOrReplaceTempView(\"new_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d7988c-89ab-410c-ae23-169864f25649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge new sales data into existing Delta Table\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_sales_data AS target\n",
    "          USING new_sales_data AS source\n",
    "          ON target.OrderID = source.OrderID\n",
    "          WHEN MATCHED THEN UPDATE SET target.OrderDate=source.OrderDate, target.CustomerID = source.CustomerID, target.Product=source.Product,\n",
    "          target.Quantity=source.Quantity, target.Price=source.Price\n",
    "          WHEN NOT MATCHED THEN INSERT (OrderID,OrderDate,CustomerID,Product,Quantity,Price)\n",
    "          VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf49897-66d9-430e-8bc2-2230b0efa1ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n",
      "|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n",
      "|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n",
      "|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n",
      "|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n",
      "|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n",
      "|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n",
      "|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n",
      "|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n",
      "|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM delta_sales_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c90ea37-e9a8-4449-9a74-5e6437624235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register delta table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_sales_table USING DELTA LOCATION '/delta/sales_data1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "410db3d9-ebf7-4f97-9a43-5e925acafe19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Optimize Delta Table\n",
    "# 3.1 Apply the OPTIMIZE command on the Delta Table and use Z-Ordering on an appropriate column.\n",
    "spark.sql(\" OPTIMIZE delta_sales_table \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9064f310-6f52-442c-a628-9eb7f9c74d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE delta_sales_table ADD COLUMN CustomerID STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70f6635-0f04-4eb0-88ad-a7d78f03d5ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"OPTIMIZE delta_sales_table ZORDER BY (CustomerID)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e61abb-f96c-4b97-a4ff-17bb3d874a15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+----------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------+------------+------------------------------------------+\n",
      "|version|timestamp          |userId         |userName                          |operation   |operationParameters                                                                                                                                     |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics|userMetadata|engineInfo                                |\n",
      "+-------+-------------------+---------------+----------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------+------------+------------------------------------------+\n",
      "|1      |2024-09-13 10:15:08|538936938566393|azuser2126_mml.local@techademy.com|ADD COLUMNS |{columns -> [{\"column\":{\"name\":\"CustomerID\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}}]}                                                           |NULL|{1385964171875517}|0911-105728-df80by3k|0          |WriteSerializable|true         |{}              |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "|0      |2024-09-13 10:14:56|538936938566393|azuser2126_mml.local@techademy.com|CREATE TABLE|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{1385964171875517}|0911-105728-df80by3k|NULL       |WriteSerializable|true         |{}              |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n",
      "+-------+-------------------+---------------+----------------------------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------+------------+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Advanced Features\n",
    "# 1. Use DESCRIBE HISTORY to inspect the history of changes for a Delta Table.\n",
    "spark.sql(\"DESCRIBE HISTORY delta_sales_table\").show(truncate=False)\n",
    "# 2. Use VACUUM to remove old files from the Delta Table.\n",
    "spark.sql(\"\"\"\n",
    "         VACUUM delta_sales_table RETAIN 168 HOURS\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ebb685-d94f-4e18-a49a-d9864970aa07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-----+\n",
      "|      Date|Region| Product|Quantity|Price|\n",
      "+----------+------+--------+--------+-----+\n",
      "|2024-09-01| North|Widget A|      10|25.50|\n",
      "|2024-09-01| South|Widget B|       5|15.75|\n",
      "|2024-09-02| North|Widget A|      12|25.50|\n",
      "|2024-09-02|  East|Widget C|       8|22.50|\n",
      "|2024-09-03|  West|Widget A|      15|25.50|\n",
      "|2024-09-03| South|Widget B|      20|15.75|\n",
      "|2024-09-03|  East|Widget C|      10|22.50|\n",
      "|2024-09-04| North|Widget D|       7|30.00|\n",
      "|2024-09-04|  West|Widget B|       9|15.75|\n",
      "+----------+------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Hands-on Exercises\n",
    "# 1. Using Delta Lake for Data Versioning\n",
    "# Query historical versions of the Delta Table using Time Travel.\n",
    "historical_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/delta/sales_data\")\n",
    "historical_df.show()\n",
    "# 2. Building a Reliable Data Lake with Delta Lake:\n",
    "# Implement schema enforcement and handle data updates with Delta Lake\n",
    "# Optimize data layout and perform vacuum operations to maintain storage efficiency.\n",
    "new_sales_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/path/to/delta/sales_data1\")\n",
    "spark.sql(\"VACUUM delta_sales_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1925364f-b1d1-4dc5-8924-e5c12028a033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hands On Delta Lake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
