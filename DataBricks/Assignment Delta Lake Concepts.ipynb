{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23e27d7-04aa-4626-93fa-4b0c5a204fe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n",
      "|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "|       101|        John|         HR| 2023-01-10| 50000|\n",
      "|       102|       Alice|    Finance| 2023-02-15| 70000|\n",
      "|       103|        Mark|Engineering| 2023-03-20| 85000|\n",
      "|       104|        Emma|      Sales| 2023-04-01| 55000|\n",
      "|       105|        Liam|  Marketing| 2023-05-12| 60000|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Creating Delta Table using Three Methods\n",
    "# 1. Load the given CSV and JSON datasets into Databricks.\n",
    "# 2. Create a Delta table using the following three methods:\n",
    "# Create a Delta table from a DataFrame.\n",
    "# Use SQL to create a Delta table.\n",
    "# Convert both the CSV and JSON files into Delta format.\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee.csv\", \"dbfs:/FileStore/employee.csv\")\n",
    "# Load the file from DBFS\n",
    "df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee.csv\")\n",
    "df_csv.show()\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/products.json\", \"dbfs:/FileStore/products.json\")\n",
    "\n",
    "# Load the file from DBFS\n",
    "df_json = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/products.json\")\n",
    "df_json.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbdf96a-0d68-47d2-acd7-653fa85aeac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/employees_delta\")\n",
    "\n",
    "df_csv.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE employees_deltasql\n",
    "USING DELTA\n",
    "AS SELECT * FROM employees_view\n",
    "\"\"\")\n",
    "\n",
    "df_csv.write.format(\"delta\").mode(\"overwrite\").save(\"/Workspace/Shared/employees_delta_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d71978b-2bdf-4b24-9447-40a8aa8bf6c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+-----+\n",
      "|ProductID|ProductName|Category|Price|\n",
      "+---------+-----------+--------+-----+\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "|     NULL|       NULL|    NULL| NULL|\n",
      "+---------+-----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/products.json\",\"dbfs:/FileStore/products.json\")\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "#Define schema for JSON file\n",
    "schema=StructType([\n",
    "    StructField(\"ProductID\",StringType(),True),\n",
    "    StructField(\"ProductName\",StringType(),True),\n",
    "    StructField(\"Category\",StringType(),True),\n",
    "    StructField(\"Price\",IntegerType(),True),\n",
    "])\n",
    "\n",
    "#Load JSON data with schema\n",
    "\n",
    "df_product=spark.read.format(\"json\").schema(schema).load(\"/FileStore/products.json\")\n",
    "df_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6903d1ca-8764-418b-a01a-953a30b2e3e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 2: Merge and Upsert (Slowly Changing Dimension - SCD)\n",
    "# 1. Load the Delta table for employees created in Task 1.\n",
    "# 2. Merge the new employee data into the employees Delta table.\n",
    "# 3. If an employee exists, update their salary. If the employee is new, insert\n",
    "# their details.\n",
    "employees_delta = spark.read.format(\"delta\").load(\"/Workspace/Shared/employees_delta\")\n",
    "\n",
    "# New employee data\n",
    "new_employee_data = [(102, \"Alice\", \"Finance\", \"2023-02-15\", 75000),  # Updated Salary\n",
    "                     (106, \"Olivia\", \"HR\", \"2023-06-10\", 65000)]  # New Employee\n",
    "\n",
    "columns = [\"EmployeeID\", \"EmployeeName\", \"Department\", \"JoiningDate\", \"Salary\"]\n",
    "new_employees_df = spark.createDataFrame(new_employee_data, columns)\n",
    "\n",
    "# Merge using Delta Lake's merge functionality\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/Workspace/Shared/employees_delta\")\n",
    "\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    new_employees_df.alias(\"src\"),\n",
    "    \"tgt.EmployeeID = src.EmployeeID\"\n",
    ").whenMatchedUpdate(set={\"Salary\": \"src.Salary\"}) \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c23fbe-1dd2-427a-938e-da465636e6da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|         userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+---------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      3|2024-09-17 05:14:37|538936938566393|azuser2126_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{4178872502868695}|0917-044935-aaw3fra4|          2|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n",
      "|      2|2024-09-17 05:00:36|538936938566393|azuser2126_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{4178872502868695}|0917-044935-aaw3fra4|          1|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|      1|2024-09-17 04:59:36|538936938566393|azuser2126_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{4178872502868695}|0917-044935-aaw3fra4|          0|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|      0|2024-09-17 04:56:23|538936938566393|azuser2126_mml.lo...|    WRITE|{mode -> ErrorIfE...|NULL|{4178872502868695}|0917-044935-aaw3fra4|       NULL|WriteSerializable|         true|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "+-------+-------------------+---------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Internals of Delta Table\n",
    "delta_table.history().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744e27e8-c7df-42c3-927f-f4499148bc33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n",
      "|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "|       101|        John|         HR| 2023-01-10| 50000|\n",
      "|       102|       Alice|    Finance| 2023-02-15| 70000|\n",
      "|       103|        Mark|Engineering| 2023-03-20| 85000|\n",
      "|       104|        Emma|      Sales| 2023-04-01| 55000|\n",
      "|       105|        Liam|  Marketing| 2023-05-12| 60000|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_version = delta_table.history(1).select(\"version\").collect()[0][0] - 1\n",
    "\n",
    "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).load(\"/Workspace/Shared/employees_delta\")\n",
    "df_time_travel.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a5191be-4bd1-42d5-b3b4-4e7f6d57defb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 4: Optimize Delta Table\n",
    "spark.sql(\"OPTIMIZE delta.`/Workspace/Shared/employees_delta`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5b19d3-a25f-4481-b841-49d45dd64ff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"OPTIMIZE delta.`/Workspace/Shared/employees_delta` ZORDER BY (Department)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35adf952-f58e-4af5-bec8-082a407fbfe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n",
      "|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "|       101|        John|         HR| 2023-01-10| 50000|\n",
      "|       102|       Alice|    Finance| 2023-02-15| 70000|\n",
      "|       103|        Mark|Engineering| 2023-03-20| 85000|\n",
      "|       104|        Emma|      Sales| 2023-04-01| 55000|\n",
      "|       105|        Liam|  Marketing| 2023-05-12| 60000|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Time Travel with Delta Table\n",
    "df_time_travel.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f3b6ef-7428-40bf-a0a0-5bb52777fc3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n",
      "|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "|       101|        John|         HR| 2023-01-10| 50000|\n",
      "|       102|       Alice|    Finance| 2023-02-15| 70000|\n",
      "|       103|        Mark|Engineering| 2023-03-20| 85000|\n",
      "|       104|        Emma|      Sales| 2023-04-01| 55000|\n",
      "|       105|        Liam|  Marketing| 2023-05-12| 60000|\n",
      "+----------+------------+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "specific_version = 1  \n",
    "df_specific_version = spark.read.format(\"delta\").option(\"versionAsOf\", specific_version).load(\"/Workspace/Shared/employees_delta\")\n",
    "df_specific_version.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1677dcbc-0003-49fe-ac4e-c9295516ceea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 6: Vacuum Delta Table\n",
    "# 1. Use the vacuum operation on the employees Delta table to remove old versions\n",
    "# and free up disk space.\n",
    "# 2. Set the retention period to 7 days and ensure that old files are deleted.\n",
    "spark.sql(\"VACUUM delta.`/Workspace/Shared/employees_delta` \")\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "spark.sql(\"VACUUM delta.`/Workspace/Shared/employees_delta` RETAIN 168 HOURS\")  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment Delta Lake Concepts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
