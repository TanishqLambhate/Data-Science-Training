{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jovpXGLPgdPh",
        "outputId": "d45d7e91-373c-4e8a-be86-3124cd4f06fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=893486087e8b9b46a2e08180cad158252581f0f1b8640d7a00b13c714d71b3d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnUrDq0XgQnP",
        "outputId": "8b735e71-f281-4102-97c8-dfc8b3abc4fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- TransactionID: string (nullable = true)\n",
            " |-- TransactionDate: string (nullable = true)\n",
            " |-- ProductID: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,to_timestamp,expr\n",
        "\n",
        "#Initialize SparkSession\n",
        "spark=SparkSession.builder.appName(\"Structured Streaming Example\").getOrCreate()\n",
        "date_format = \"yyyy-MM-dd\"\n",
        "#Define the schema for the csv data\n",
        "TRANSACTION_schema=\"TransactionID STRING , TransactionDate STRING , ProductID STRING , Quantity INT ,Price DOUBLE\"\n",
        "\n",
        "#Read streaming data from csv files\n",
        "df_transaction_stream=spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "        .option(\"header\",\"true\") \\\n",
        "            .schema(TRANSACTION_schema) \\\n",
        "                .load(\"/content/sample_data/\")\n",
        "\n",
        "df_transaction_stream.printSchema()\n",
        "df_with_timestamp = df_transaction_stream.withColumn(\"TransactionDate\", to_timestamp(\"TransactionDate\", date_format))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjiLXTn6hnJ-",
        "outputId": "bccc195f-2fe4-407e-feb2-582df26d9771"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.query.StreamingQuery at 0x7b9842ce6a10>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Task 1: Ingest Streaming Data from CSV Files\n",
        "# 1. Create a folder for streaming CSV files.\n",
        "# 2. Set up a structured streaming source to continuously read CSV data from this\n",
        "# folder.\n",
        "\n",
        "# 3. Ensure that the streaming query reads the data continuously in append mode and\n",
        "# displays the results in the console.\n",
        "df_with_timestamp.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "        .outputMode(\"append\") \\\n",
        "            .start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7WNa0hyYih4J"
      },
      "outputs": [],
      "source": [
        "# Task 2: Stream Transformations\n",
        "# 1. Once the data is streaming in, perform transformations on the incoming data:\n",
        "# Add a new column for the TotalAmount ( Quantity * Price ).\n",
        "# Filter records where the Quantity is greater than 1.\n",
        "transformed_df = df_with_timestamp.withColumn(\"TotalAmount\", df_with_timestamp[\"Quantity\"] * df_with_timestamp[\"Price\"]) \\\n",
        "                             .filter(df_with_timestamp[\"Quantity\"] > 1)\n",
        "\n",
        "\n",
        "watermarked_df = transformed_df \\\n",
        "    .withWatermark(\"TransactionDate\", \"1 day\") \\\n",
        "    .groupBy(\"ProductID\") \\\n",
        "    .agg(expr(\"sum(TotalAmount) as TotalSales\"))\n",
        "# 2. Write the transformed stream to a memory sink to see the updated results\n",
        "# continuously.\n",
        "# spark.streams.active[0].stop()\n",
        "query = transformed_df.writeStream \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"transformed_stream\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fsRZvyXlm4G",
        "outputId": "456c6aae-e430-498a-c7d1-76c948ea88dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+-------------------+---------+--------+------+-----------+\n",
            "|TransactionID|TransactionDate    |ProductID|Quantity|Price |TotalAmount|\n",
            "+-------------+-------------------+---------+--------+------+-----------+\n",
            "|T101         |2024-01-01 00:00:00|Laptop   |2       |1200.0|2400.0     |\n",
            "|T103         |2024-01-03 00:00:00|Tablet   |3       |600.0 |1800.0     |\n",
            "|T105         |2024-01-05 00:00:00|Mouse    |5       |25.0  |125.0      |\n",
            "+-------------+-------------------+---------+--------+------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT * FROM transformed_stream\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y7oFQK_TmKc4"
      },
      "outputs": [],
      "source": [
        "# Task 3: Aggregations on Streaming Data\n",
        "# 1. Implement an aggregation on the streaming data:\n",
        "# Group the data by ProductID and calculate the total sales for each\n",
        "\n",
        "# Use the transformed_df DataFrame which contains the TotalAmount column\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "aggregated_df = transformed_df.groupBy(\"ProductID\") \\\n",
        "                            .agg(F.sum(transformed_df[\"Quantity\"] * transformed_df[\"Price\"]).alias(\"TotalSales\"))\n",
        "\n",
        "\n",
        "# product (i.e., sum of Quantity * Price for each product).\n",
        "# 2. Ensure the stream runs in update mode, so only updated results are output to\n",
        "# the sink.\n",
        "\n",
        "memory_query = transformed_df.writeStream \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"product_sales\") \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khC5TpDkLdQR"
      },
      "outputs": [],
      "source": [
        "# Task 4: Writing Streaming Data to File Sinks\n",
        "# 1. After transforming and aggregating the data, write the streaming results to a\n",
        "# Parquet sink.\n",
        "# 2. Ensure that you configure a checkpoint location to store progress and ensure\n",
        "# recovery in case of failure.\n",
        "parquet_query = aggregated_df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", \"/mnt/streaming_output/\") \\\n",
        "    .option(\"checkpointLocation\", \"/mnt/checkpoints/\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5PsH-BEWqn4o"
      },
      "outputs": [],
      "source": [
        "# Task 5: Handling Late Data using Watermarks\n",
        "# 1. Introduce a watermark on the TransactionDate column to handle late data\n",
        "# arriving in the stream.\n",
        "\n",
        "# 2. Set the watermark to 1 day to allow late data within a 24-hour period and\n",
        "# discard data that is older.\n",
        "\n",
        "watermark_df = transformed_df.withWatermark(\"TransactionDate\", \"1 day\")\n",
        "watermarked_aggregated_df = watermark_df.groupBy(\"ProductID\") \\\n",
        "                                        .agg({\"TotalAmount\": \"sum\"}) \\\n",
        "                                        .withColumnRenamed(\"sum(TotalAmount)\", \"TotalSales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvkMzeFlLZOi"
      },
      "outputs": [],
      "source": [
        "# Task 6: Streaming from Multiple Sources\n",
        "# 1. Simulate a scenario where two streams of data are being ingested:\n",
        "# Stream 1: Incoming transaction data (same as Task 1).\n",
        "# Stream 2: Product information (CSV with columns: ProductID, ProductName,\n",
        "# Category).\n",
        "# 2. Perform a join on the two streams using the ProductID column and display the\n",
        "# combined stream results.\n",
        "product_schema = \"ProductID STRING, ProductName STRING, Category STRING\"\n",
        "\n",
        "product_stream = spark.readStream.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(product_schema) \\\n",
        "    .load(\"/product_data/\")\n",
        "\n",
        "joined_stream = watermarked_df.join(product_stream, \"ProductID\")\n",
        "\n",
        "joined_query = joined_stream.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "joined_query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o42nZ70LLUS_"
      },
      "outputs": [],
      "source": [
        "# Task 7: Stopping and Restarting Streaming Queries\n",
        "# 1. Stop the streaming query and explore the results.\n",
        "# 2. Restart the query and ensure that it continues from the last processed data by\n",
        "# utilizing the checkpoint.\n",
        "restarted_query = transformed_df.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .option(\"checkpointLocation\", \"/mnt/checkpoints/\") \\\n",
        "    .start()\n",
        "\n",
        "restarted_query.awaitTermination()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
