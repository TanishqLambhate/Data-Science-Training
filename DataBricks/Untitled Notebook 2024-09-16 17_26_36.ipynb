{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f6b31c-91c2-4a7a-bae0-5229d104911d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Delta table created and data written successfully.\nNew data appended to Delta table successfully.\nMerging new data into Delta table...\nData merged successfully.\nViewing Delta table history...\n+-------+-------------------+---------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId         |userName                          |operation|operationParameters                                                                                                                                                                                |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |userMetadata|engineInfo                                |\n+-------+-------------------+---------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|2      |2024-09-16 12:02:57|538936938566393|azuser2126_mml.local@techademy.com|MERGE    |{predicate -> [\"(ID#683L = ID#403L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{1877205108496512}|0911-105728-df80by3k|1          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 1240, numTargetBytesRemoved -> 1772, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 6740, materializeSourceTimeMs -> 396, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 4889, numTargetRowsUpdated -> 3, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 3, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1362}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2024-09-16 12:02:45|538936938566393|azuser2126_mml.local@techademy.com|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                          |NULL|{1877205108496512}|0911-105728-df80by3k|0          |WriteSerializable|true         |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1181}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2024-09-16 12:02:37|538936938566393|azuser2126_mml.local@techademy.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                       |NULL|{1877205108496512}|0911-105728-df80by3k|NULL       |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 1773}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+---------------+----------------------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\nQuerying Delta table as of version 0...\n+---+-----+\n|ID |Value|\n+---+-----+\n|1  |100  |\n|2  |200  |\n|3  |300  |\n+---+-----+\n\nVacuuming old files...\nDelta operations completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaOperationsSimpleExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Delta table path\n",
    "delta_table_path = \"/delta/simple_data\"\n",
    "\n",
    "# Define initial sample data\n",
    "initial_data = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 300)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"ID\", \"Value\"]\n",
    "\n",
    "# Create DataFrame for initial data\n",
    "df_initial = spark.createDataFrame(initial_data, schema=schema)\n",
    "\n",
    "# Write DataFrame to Delta table\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Initial Delta table created and data written successfully.\")\n",
    "\n",
    "# Define new sample data\n",
    "new_sample_data = [\n",
    "    (2, 250),  # Existing ID with updated Value\n",
    "    (4, 400)   # New ID\n",
    "]\n",
    "\n",
    "# Create DataFrame for new data\n",
    "df_new = spark.createDataFrame(new_sample_data, schema=schema)\n",
    "\n",
    "# Write the new data to Delta table in append mode\n",
    "df_new.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "print(\"New data appended to Delta table successfully.\")\n",
    "\n",
    "# Create a temporary view for SQL operations\n",
    "df_new.createOrReplaceTempView(\"new_data\")\n",
    "\n",
    "# Perform the merge operation\n",
    "print(\"Merging new data into Delta table...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO delta.`{delta_table_path}` AS target\n",
    "USING new_data AS source\n",
    "ON target.ID = source.ID\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.Value = source.Value\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    ID,\n",
    "    Value\n",
    ") VALUES (\n",
    "    source.ID,\n",
    "    source.Value\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data merged successfully.\")\n",
    "\n",
    "# Delta operations - History, Time Travel, and Vacuum\n",
    "print(\"Viewing Delta table history...\")\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "print(\"Querying Delta table as of version 0...\")\n",
    "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "df_time_travel.show(truncate=False)\n",
    "\n",
    "print(\"Vacuuming old files...\")\n",
    "spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 168 HOURS\")\n",
    "\n",
    "print(\"Delta operations completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-09-16 17:26:36",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
