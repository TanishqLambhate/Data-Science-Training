{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f811789-b413-41e6-90fc-5cd569f21a38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\nroot\n |-- EmployeeID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- JoiningDate: date (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Load employee.csv file data\n",
    "df_employee = spark.read.csv('/FileStore/employee_data.csv', header=True, inferSchema=True).cache()\n",
    "df_employee.show()\n",
    "df_employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4060383-e8c1-458c-983e-dcdf937d26ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,DoubleType\n",
    "\n",
    "# Define the schema\n",
    "schema=StructType([\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"ProductName\", StringType(),True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Stock\",IntegerType(),True)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576d65fb-06f6-49a2-80e2-5d2813dec98e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------+-----+\n|ProductID|ProductName|   Category| Price|Stock|\n+---------+-----------+-----------+------+-----+\n|      101|     Laptop|Electronics|1200.0|   35|\n|      102| Smartphone|Electronics| 800.0|   80|\n|      103| Desk Chair|  Furniture| 150.0|   60|\n|      104|    Monitor|Electronics| 300.0|   45|\n|      105|       Desk|  Furniture| 350.0|   25|\n+---------+-----------+-----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Load json data with schema\n",
    "df_product=spark.read.format(\"json\").schema(schema).load(\"/FileStore/product_data.json\")\n",
    "df_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d383204f-e9d0-4eef-ba21-0ed0d8c61bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Convert CSV and JSON Data to Delta Format\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/delta/employee_data\")\n",
    "\n",
    "# create temp view for sql operations\n",
    "df_product.createOrReplaceTempView(\"product_view\")\n",
    "# create delta table from view\n",
    "spark.sql(\"CREATE TABLE if not EXISTS delta_product_table USING DELTA AS SELECT * FROM product_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd32f589-d03a-4439-b3e8-ee5d5e3402d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Register Delta Tables as SQL Tables\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_employee_table USING DELTA \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c6b86d-e0f4-4657-a467-2f6aaa67724c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Data Modifications with Delta Tables\n",
    "# Increase salary by 5% for IT department employees\n",
    "spark.sql(\"UPDATE employee_delta SET Salary = Salary * 1.05 WHERE Department = 'IT'\")\n",
    "# Delete products where stock is less than 40\n",
    "spark.sql(\"DELETE FROM product_delta WHERE Stock < 40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe0eb58-2f73-4dad-a1c2-8d5b4dd68d60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n|  Furniture|  350|      105|       Desk|   25|\n+-----------+-----+---------+-----------+-----+\n\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Time Travel with Delta Tables:\n",
    "# Query the product Delta table to show its state before the delete\n",
    "# operation (use time travel).\n",
    "df_product_version_before_delete = spark.sql(\"SELECT * FROM product_delta VERSION AS OF 0\")\n",
    "df_product_version_before_delete.show()\n",
    "# Retrieve the version of the employee Delta table before the salary update.\n",
    "df_employee_version_before_update = spark.sql(\"SELECT * FROM employee_delta VERSION AS OF 0\")\n",
    "df_employee_version_before_update.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23772e80-2cc1-42d8-8eb8-d1feb1403476",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n+----------+-------------+----------+-----------+------+\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics|  800|      102| Smartphone|   80|\n+-----------+-----+---------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Query Delta Tables:\n",
    "# Query the employee Delta table to find the employees in the Finance department.\n",
    "df_finance_employees = spark.sql(\"SELECT * FROM employee_delta WHERE Department = 'Finance'\")\n",
    "df_finance_employees.show()\n",
    "# Query the product Delta table to find all products in the Electronics category with a price greater than 500.\n",
    "df_expensive_electronics = spark.sql(\"SELECT * FROM product_delta WHERE Category = 'Electronics' AND Price > 500\")\n",
    "df_expensive_electronics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3ae9e9-9545-443b-a247-e7ab4b055b31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/employees_updates.csv\", \"dbfs:/FileStore/employees_updates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97faf91-434d-4436-95aa-9f15dc9c3594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employee=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/FileStore/employee_data.csv\")\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_data\")\n",
    "\n",
    "#convert employee updates csv to delta format\n",
    "df_employee_updates=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/FileStore/employees_updates.csv\")\n",
    "df_employee_updates.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_updates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0a9d7e-8ed5-4b21-9b75-d366309479c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load delta tables\n",
    "df_employee=spark.read.format(\"delta\").load(\"/delta/employee_data\")\n",
    "df_employee_updates=spark.read.format(\"delta\").load(\"/delta/employee_updates\")\n",
    "\n",
    "# Create temporary views for SQL operations\n",
    "df_employee.createOrReplaceTempView(\"delta_employee\")\n",
    "df_employee_updates.createOrReplaceTempView(\"employee_updates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ec155c-4b28-42d7-81e4-e7dfa2b2a11c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          MERGE INTO delta_employee AS target\n",
    "          USING employee_updates AS source\n",
    "          ON target.EmployeeID = source.EmployeeID\n",
    "          WHEN MATCHED THEN UPDATE SET target.salary = source.Salary, target.Department=source.Department\n",
    "          WHEN NOT MATCHED THEN INSERT (EmployeeID, Name, Department, JoiningDate, Salary)\n",
    "          VALUES (source.EmployeeID, source.Name, source.Department, source.JoiningDate, source.Salary)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faae3372-ce24-49b6-95d7-e5c8ac8be75c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n|      1001|     John Doe|        HR| 2021-01-15| 58000|\n|      1009|  Sarah Adams| Marketing| 2021-09-01| 60000|\n|      1010|  Robert King|        IT| 2022-01-10| 62000|\n+----------+-------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Query the Delta table to check if the data was updated or inserted correctly\n",
    "spark.sql(\"SELECT * FROM delta_employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1950e3-355a-4f26-991c-6b718867ca65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write the employee dataframe to a delta table\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e999fc1c-04cd-44f6-b027-a86df3bca1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register delta table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS delta_employee_table USING DELTA LOCATION '/delta/employee_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f862a659-6216-46bb-8f43-f7e8eba3b100",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize the delta table\n",
    "spark.sql(\"OPTIMIZE delta_employee_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d56972-72da-4790-81c1-ffba24d50b1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation   |operationParameters                                                                                                                    |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics|userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------+------------+------------------------------------------+\n|0      |2024-09-12 10:21:26|8838676295264951|azuser2125_mml.local@techademy.com|CREATE TABLE|{partitionBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{1933672499065146}|0911-091631-qlq2ghxz|NULL       |WriteSerializable|true         |{}              |NULL        |Databricks-Runtime/14.3.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+------------+---------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Describe the history of delta table\n",
    "spark.sql(\"DESCRIBE HISTORY delta_employee_table\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Working with delta tables",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
