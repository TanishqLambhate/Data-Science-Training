{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt7OJjLzqMyhq1+aoHoV/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanishqLambhate/Data-Science-Training/blob/pyspark/Pyspark_Excercise_3_RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rzh9asDTZHH",
        "outputId": "a9a90619-eada-4d17-ccbf-7fead24a9cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=eb04db76e50f27705ccb5b587e3fdae4be68213eec70bdd9cdbc68f60e840898\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = [\n",
        "    (\"ProductA\", 100),\n",
        "    (\"ProductB\", 150),\n",
        "    (\"ProductA\", 200),\n",
        "    (\"ProductC\", 300),\n",
        "    (\"ProductB\", 250),\n",
        "    (\"ProductC\", 100)\n",
        "]\n",
        "regional_sales_data = [\n",
        "    (\"ProductA\", 50),\n",
        "    (\"ProductC\", 150)\n",
        "]\n",
        "### **Step 1: Initialize Spark Context**\n",
        "\n",
        "# 1. **Initialize SparkSession and SparkContext:**\n",
        "#    - Create a Spark session in PySpark and use the `spark.sparkContext` to create an RDD from the provided data.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"RDD for sales and regional data\") .getOrCreate()\n",
        "\n",
        "# 1. **Initialize SparkSession and SparkContext:**\n",
        "#    - Create a Spark session in PySpark and use the `spark.sparkContext` to create an RDD from the provided data.\n",
        "sc=spark.sparkContext\n",
        "print(\"Spark session created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMLS9MMPVXuG",
        "outputId": "fcd1c85f-b573-4abb-a49c-eb15049a36cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark session created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. **Task 1: Create an RDD from the Sales Data**\n",
        "#    - Create an RDD from the `sales_data` list provided above.\n",
        "#    - Print the first few elements of the RDD.\n",
        "rdd=sc.parallelize(sales_data)\n",
        "#Print the original RDD\n",
        "print(\"Original RDD:\",rdd.collect())\n",
        "\n",
        "# 3. **Task 2: Group Data by Product Name**\n",
        "#    - Group the sales data by product name using `groupByKey()`.\n",
        "#    - Print the grouped data to understand its structure.\n",
        "grouped_sales_rdd = rdd.groupByKey()\n",
        "print(\"Grouped Data by Product Name:\")\n",
        "for key, values in grouped_sales_rdd.collect():\n",
        "    print(f\"{key}: {list(values)}\")\n",
        "\n",
        "# 4. **Task 3: Calculate Total Sales by Product**\n",
        "#    - Use `reduceByKey()` to calculate the total sales for each product.\n",
        "#    - Print the total sales for each product.\n",
        "total_sales_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
        "print(\"Total Sales by Product:\")\n",
        "for product, total_sales in total_sales_rdd.collect():\n",
        "    print(f\"{product}: {total_sales}\")\n",
        "\n",
        "# 5. **Task 4: Sort Products by Total Sales**\n",
        "#    - Sort the products by their total sales in descending order.\n",
        "#    - Print the sorted list of products along with their sales amounts.\n",
        "sorted_sales_rdd = total_sales_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "print(\"Sorted Products by Total Sales:\")\n",
        "for product, total_sales in sorted_sales_rdd.collect():\n",
        "    print(f\"{product}: {total_sales}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl_-OrjTWqDX",
        "outputId": "8a2cba5e-4933-4aad-9c4a-ac2af2615c0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original RDD: [('ProductA', 100), ('ProductB', 150), ('ProductA', 200), ('ProductC', 300), ('ProductB', 250), ('ProductC', 100)]\n",
            "Grouped Data by Product Name:\n",
            "ProductA: [100, 200]\n",
            "ProductB: [150, 250]\n",
            "ProductC: [300, 100]\n",
            "Total Sales by Product:\n",
            "ProductA: 300\n",
            "ProductB: 400\n",
            "ProductC: 400\n",
            "Sorted Products by Total Sales:\n",
            "ProductB: 400\n",
            "ProductC: 400\n",
            "ProductA: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### **Step 4: Additional Transformations**\n",
        "\n",
        "# 6. **Task 5: Filter Products with High Sales**\n",
        "#    - Filter the products that have total sales greater than 200.\n",
        "#    - Print the products that meet this condition.\n",
        "high_sales_rdd = total_sales_rdd.filter(lambda x: x[1] > 200)\n",
        "print(\"Products with Total Sales Greater Than 200:\")\n",
        "for product, total_sales in high_sales_rdd.collect():\n",
        "    print(f\"{product}: {total_sales}\")\n",
        "\n",
        "# 7. **Task 6: Combine Regional Sales Data**\n",
        "#    - Create another RDD from the `regional_sales_data` list.\n",
        "#    - Combine this RDD with the original sales RDD using `union()`.\n",
        "#    - Calculate the new total sales for each product after combining the datasets.\n",
        "#    - Print the combined sales data.\n",
        "\n",
        "regional_sales_rdd = sc.parallelize(regional_sales_data)\n",
        "combined_sales_rdd = rdd.union(regional_sales_rdd)\n",
        "new_total_sales_rdd = combined_sales_rdd.reduceByKey(lambda x, y: x + y)\n",
        "print(\"Combined Sales Data (after union):\")\n",
        "for product, total_sales in new_total_sales_rdd.collect():\n",
        "    print(f\"{product}: {total_sales}\")\n",
        "\n",
        "# ### **Step 5: Perform Actions on the RDD**\n",
        "\n",
        "# 8. **Task 7: Count the Number of Distinct Products**\n",
        "#    - Count the number of distinct products in the RDD.\n",
        "#    - Print the count of distinct products.\n",
        "distinct_products_count = combined_sales_rdd.keys().distinct().count()\n",
        "print(f\"Number of Distinct Products: {distinct_products_count}\")\n",
        "\n",
        "# 9. **Task 8: Identify the Product with Maximum Sales**\n",
        "#    - Find the product with the maximum total sales using `reduce()`.\n",
        "#    - Print the product name and its total sales amount.\n",
        "max_sales_product = new_total_sales_rdd.reduce(lambda x, y: x if x[1] > y[1] else y)\n",
        "print(f\"Product with Maximum Sales: {max_sales_product[0]} with sales amount {max_sales_product[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLr0AR3paR26",
        "outputId": "49b21ee4-c05c-4c52-e55e-85e6cc14f18e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products with Total Sales Greater Than 200:\n",
            "ProductA: 300\n",
            "ProductB: 400\n",
            "ProductC: 400\n",
            "Combined Sales Data (after union):\n",
            "ProductA: 350\n",
            "ProductC: 550\n",
            "ProductB: 400\n",
            "Number of Distinct Products: 3\n",
            "Product with Maximum Sales: ProductC with sales amount 550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Challenge Task: Calculate the Average Sales per Product**\n",
        "\n",
        "# 10. **Challenge Task:**\n",
        "#     - Calculate the average sales amount per product using the key-value pair RDD.\n",
        "#     - Print the average sales for each product.\n",
        "sales_and_count_rdd = combined_sales_rdd.mapValues(lambda x: (x, 1)) \\\n",
        "                                        .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "\n",
        "average_sales_rdd = sales_and_count_rdd.mapValues(lambda x: x[0] / x[1])\n",
        "print(\"Average Sales per Product:\")\n",
        "for product, avg_sales in average_sales_rdd.collect():\n",
        "    print(f\"{product}: {avg_sales}\")"
      ],
      "metadata": {
        "id": "g5mYxCNQcmhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}