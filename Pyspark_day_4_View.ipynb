{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl/3OiYFQBeUDtc4U0SAUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanishqLambhate/Data-Science-Training/blob/pyspark/Pyspark_day_4_View.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd6_dh1RXWYb",
        "outputId": "c389d90e-3b26-4aca-ae31-4908216e78ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=a772049ac7204c83a3623e5ecef94ef2d4bd5c6a1101078af7dc08e8b8e3fbdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"Example\").getOrCreate()"
      ],
      "metadata": {
        "id": "honFQ2aXYWs7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path=\"/content/sample_data/people.csv\"\n",
        "\n",
        "#Now you can read it with pyspark\n",
        "df_csv=spark.read.format(\"csv\").option(\"header\",\"true\").load(csv_file_path)\n",
        "df_csv.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZEdlkHsXa4T",
        "outputId": "9ed6f59d-b16d-42ed-d99e-b90de10af074"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+\n",
            "|Name| Age| Gender|\n",
            "+----+----+-------+\n",
            "|John|  28|   Male|\n",
            "|Jane|  32| Female|\n",
            "+----+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define the schema for the JSON file\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"address\", StructType([\n",
        "        StructField(\"street\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "#load the complex JSON file with the correct path\n",
        "\n",
        "json_file_path=\"/content/sample_data/sample.json\"\n",
        "df_json_comlplex=spark.read.schema(schema).json(json_file_path)\n",
        "\n",
        "# Read the file as text to inspect its contents\n",
        "with open(json_file_path,'r') as f:\n",
        "  data=f.read()\n",
        "  print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkYtRgMfdpBX",
        "outputId": "da001d36-19ad-4b24-9e63-1bef412c9d4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"name\": \"John\",\n",
            "    \"age\": 28,\n",
            "    \"gender\": \"Male\",\n",
            "    \"address\": {\n",
            "      \"street\": \"123 Main St\",\n",
            "      \"city\": \"New York\"\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"name\": \"Jane\",\n",
            "    \"age\": 32,\n",
            "    \"gender\": \"Female\",\n",
            "    \"address\": {\n",
            "      \"street\": \"456 Elm St\",\n",
            "      \"city\": \"San Francisco\"\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data={\n",
        "    \"name\":[\"John\",\"Jane\",\"Mike\",\"Emily\"],\n",
        "    \"age\":[25,30,22,28],\n",
        "    \"gender\":[\"Male\",\"Female\",\"Male\",\"Female\"],\n",
        "    \"city\":[\"New York\",\"London\",\"Paris\",\"Tokyo\"]\n",
        "}\n",
        "\n",
        "df=pd.DataFrame(data)\n",
        "\n",
        "#Save the Dataframe\n",
        "csv_file_path=\"/content/sample_data/sample_people.csv\"\n",
        "df.to_csv(csv_file_path,index=False)\n",
        "\n",
        "#confirm the file has been created\n",
        "print(f\"csv file created at:{csv_file_path}\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"Create View\").getOrCreate()\n",
        "\n",
        "df_people=spark.read.format(\"csv\").option(\"header\",\"true\").load(csv_file_path)\n",
        "df_people.show()\n",
        "\n",
        "#create a temporary view\n",
        "df_people.createOrReplaceTempView(\"people_temp_view\")\n",
        "\n",
        "#Run an sql query on the view\n",
        "result_temp_view=spark.sql(\"Select name,age,city from people_temp_view where age<30\")\n",
        "\n",
        "result_temp_view.show()\n",
        "\n",
        "#create a global view\n",
        "df_people.createOrReplaceGlobalTempView(\"people_global_view\")\n",
        "\n",
        "#run an sql query on the global view\n",
        "result_global_view=spark.sql(\"Select name,age,city from global_temp.people_global_view where age<30\")\n",
        "\n",
        "result_global_view.show()\n",
        "\n",
        "#List all temprorary views and tables\n",
        "spark.catalog.listTables()\n",
        "\n",
        "#Drop a temporary view\n",
        "spark.catalog.dropTempView(\"people_temp_view\")\n",
        "\n",
        "#List all temporary views and tables again\n",
        "spark.catalog.listTables\n",
        "\n",
        "#Drop the global temprorary view\n",
        "spark.catalog.dropGlobalTempView(\"people_global_view\")\n",
        "\n",
        "#List all temporary views and tables again\n",
        "spark.catalog.listTables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wy2eNFJqlUsp",
        "outputId": "f75f75d7-48b8-4535-9f1b-9042a1482237"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv file created at:/content/sample_data/sample_people.csv\n",
            "+-----+---+------+--------+\n",
            "| name|age|gender|    city|\n",
            "+-----+---+------+--------+\n",
            "| John| 25|  Male|New York|\n",
            "| Jane| 30|Female|  London|\n",
            "| Mike| 22|  Male|   Paris|\n",
            "|Emily| 28|Female|   Tokyo|\n",
            "+-----+---+------+--------+\n",
            "\n",
            "+-----+---+--------+\n",
            "| name|age|    city|\n",
            "+-----+---+--------+\n",
            "| John| 25|New York|\n",
            "| Mike| 22|   Paris|\n",
            "|Emily| 28|   Tokyo|\n",
            "+-----+---+--------+\n",
            "\n",
            "+-----+---+--------+\n",
            "| name|age|    city|\n",
            "+-----+---+--------+\n",
            "| John| 25|New York|\n",
            "| Mike| 22|   Paris|\n",
            "|Emily| 28|   Tokyo|\n",
            "+-----+---+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Catalog.listTables of <pyspark.sql.catalog.Catalog object at 0x7f395a9c1fc0>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.catalog.Catalog.listTables</b><br/>def listTables(dbName: Optional[str]=None, pattern: Optional[str]=None) -&gt; List[Table]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/catalog.py</a>Returns a list of tables/views in the specified database.\n",
              "\n",
              ".. versionadded:: 2.0.0\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "dbName : str\n",
              "    name of the database to list the tables.\n",
              "\n",
              "    .. versionchanged:: 3.4.0\n",
              "       Allow ``dbName`` to be qualified with catalog name.\n",
              "\n",
              "pattern : str\n",
              "    The pattern that the database name needs to match.\n",
              "\n",
              "    .. versionchanged: 3.5.0\n",
              "        Adds ``pattern`` argument.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "list\n",
              "    A list of :class:`Table`.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "If no database is specified, the current database and catalog\n",
              "are used. This API includes all temporary views.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; spark.range(1).createTempView(&quot;test_view&quot;)\n",
              "&gt;&gt;&gt; spark.catalog.listTables()\n",
              "[Table(name=&#x27;test_view&#x27;, catalog=None, namespace=[], description=None, ...\n",
              "\n",
              "&gt;&gt;&gt; spark.catalog.listTables(pattern=&quot;test*&quot;)\n",
              "[Table(name=&#x27;test_view&#x27;, catalog=None, namespace=[], description=None, ...\n",
              "\n",
              "&gt;&gt;&gt; spark.catalog.listTables(pattern=&quot;table*&quot;)\n",
              "[]\n",
              "\n",
              "&gt;&gt;&gt; _ = spark.catalog.dropTempView(&quot;test_view&quot;)\n",
              "&gt;&gt;&gt; spark.catalog.listTables()\n",
              "[]</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 310);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a new database in Spark SQL\n",
        "# spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
        "\n",
        "# # Use the created database\n",
        "# spark.sql(\"USE my_database\")\n",
        "\n",
        "# # Verify that the database is being used\n",
        "# spark.sql(\"SHOW DATABASES\").show()"
      ],
      "metadata": {
        "id": "c7Bw2VQq8WwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new database in Spark SQL\n",
        "\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
        "\n",
        "# Use the created database\n",
        "\n",
        "spark.sql(\"USE my_database\")\n",
        "\n",
        "# Verify that the database is being used\n",
        "\n",
        "spark.sql(\"SHOW DATABASES\").show()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample CSV data\n",
        "\n",
        "data = {\n",
        "\n",
        "    \"name\": [\"John\", \"Jane\", \"Mike\", \"Emily\", \"Alex\"],\n",
        "\n",
        "    \"age\": [28, 32, 45, 23, 36],\n",
        "\n",
        "    \"gender\": [\"Male\", \"Female\", \"Male\", \"Female\", \"Male\"],\n",
        "\n",
        "    \"salary\": [60000, 72000, 84000, 52000, 67000]\n",
        "\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "\n",
        "csv_file_path = \"/content/sample_people.csv\"\n",
        "\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Confirm the CSV file is created\n",
        "\n",
        "print(f\"CSV file created at: {csv_file_path}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "zFp5453F8ctx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}